<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>üé§ Realtime STT Debug with PCM streaming & debug</title>
  <style>
    body { font-family: monospace; background: #f4f4f4; padding: 20px; }
    #log { background: #fff; padding: 10px; border: 1px solid #ccc; height: 400px; overflow-y: scroll; white-space: pre-wrap; }
  </style>
</head>
<body>
  <h1>üîä Realtime STT Debug (with detailed logs)</h1>
  <button id="start">Start Streaming</button>
  <pre id="log"></pre>

  <script>
    const logEl = document.getElementById('log');
    const log = (msg) => {
      console.log(msg);
      logEl.textContent += msg + '\n';
      logEl.scrollTop = logEl.scrollHeight;
    };

    let ws;
    let audioContext;
    let processor;
    let source;

    function convertFloat32ToInt16(buffer) {
      const l = buffer.length;
      const buf = new Int16Array(l);
      for (let i = 0; i < l; i++) {
        let s = Math.max(-1, Math.min(1, buffer[i]));
        buf[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      return buf.buffer;
    }

    async function startStreaming() {
      if (ws && ws.readyState === WebSocket.OPEN) {
        log('‚ö†Ô∏è WebSocket already open');
        return;
      }

      try {
        ws = new WebSocket('wss://speech-app-server.onrender.com');
        ws.binaryType = 'arraybuffer';

        ws.onopen = async () => {
          log('‚úÖ WebSocket opened');
          try {
            audioContext = new AudioContext({ sampleRate: 16000 });
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            source = audioContext.createMediaStreamSource(stream);

            processor = audioContext.createScriptProcessor(4096, 1, 1);

            processor.onaudioprocess = (e) => {
              const floatData = e.inputBuffer.getChannelData(0);
              const int16Data = convertFloat32ToInt16(floatData);
              if (ws.readyState === WebSocket.OPEN) {
                ws.send(int16Data);
                log(`‚¨Ü Sent audio chunk (${int16Data.byteLength} bytes)`);
              } else {
                log('‚ö†Ô∏è WebSocket not open while sending audio');
              }
            };

            source.connect(processor);
            processor.connect(audioContext.destination);

            log('üéôÔ∏è AudioContext and processor started');
          } catch (err) {
            log('‚ùå Error accessing microphone or AudioContext: ' + err.message);
          }
        };

        ws.onmessage = (event) => {
          try {
            const msg = JSON.parse(event.data);
            if (msg.type === 'keepalive') return;

            const type = msg.is_final ? '‚úÖ Final' : '‚è≥ Interim';
            log(`‚¨á ${type}: "${msg.transcript}"`);
          } catch (e) {
            log('‚ö†Ô∏è Non-JSON message: ' + event.data);
          }
        };

        ws.onerror = (err) => {
          log('‚ùå WebSocket error: ' + err.message);
        };

        ws.onclose = () => {
          log('üîå WebSocket closed');
          if (processor) processor.disconnect();
          if (source) source.disconnect();
          if (audioContext) audioContext.close();
        };
      } catch (err) {
        log('‚ùå Exception in startStreaming: ' + err.message);
      }
    }

    document.getElementById('start').onclick = startStreaming;
  </script>
</body>
</html>
